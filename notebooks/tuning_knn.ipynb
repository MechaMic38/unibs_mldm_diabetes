{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Tuning | SVM\n",
    "**SVM** (Classifier) hyper-parameter tuning for the [UCI dataset](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008). The dataset represents ten years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks.\n",
    "\n",
    "> **[CAUTION]** Do <u>NOT</u> execute \"Run All\", that will tell Optuna to run hyper-parameter tuning, regardless of whether you have already done it or not. Avoid running the _\"Hyper-parameter tuning\"_ subsections if you don't have to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Initial Setup**\n",
    "Taking care of package imports, defining work constants, and loading all necessary datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "# Machine learning\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualization configurations\n",
    "pd.set_option('display.max_columns', 60)\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, fbeta_score, make_scorer, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/original/'\n",
    "DATA_PATH_PROCESSED = '../data/processed/'\n",
    "\n",
    "MODEL_NAME = 'svm' # Modify this to the model you are using\n",
    "\n",
    "OPTUNA_PATH = '../optuna/'\n",
    "OPTUNA_DIR = OPTUNA_PATH + MODEL_NAME + '/'\n",
    "\n",
    "STUDY_NAME = \"Diabetes_130-US\"\n",
    "STUDY_PATH_NONE = OPTUNA_DIR + f\"optuna_{MODEL_NAME}_none.log\"\n",
    "STUDY_PATH_RUS = OPTUNA_DIR + f\"optuna_{MODEL_NAME}_rus.log\"\n",
    "STUDY_PATH_SMOTE = OPTUNA_DIR + f\"optuna_{MODEL_NAME}_smote.log\"\n",
    "STUDY_PATH_SMOTE_TOMEK = OPTUNA_DIR + f\"optuna_{MODEL_NAME}_smote-tomek.log\"\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(OPTUNA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(STUDY_PATH_NONE), exist_ok=True)\n",
    "\n",
    "N_TRIALS = 100\n",
    "RANDOM_STATE = 38\n",
    "\n",
    "SCORE_NAME = 'F1-Score'\n",
    "SCORING = make_scorer(fbeta_score, beta=1, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\n",
    "    DATA_PATH_PROCESSED + 'train.csv',\n",
    "    na_values='?',\n",
    "    keep_default_na=False\n",
    ")\n",
    "\n",
    "test_set = pd.read_csv(\n",
    "    DATA_PATH_PROCESSED + 'test.csv',\n",
    "    na_values='?',\n",
    "    keep_default_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Feature Scaling**\n",
    "Applying scaling to numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'readmitted'\n",
    "\n",
    "X_train = train_set.drop(target, axis=1)\n",
    "y_train = train_set[target]\n",
    "\n",
    "X_test = test_set.drop(target, axis=1)\n",
    "y_test = test_set[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Scaling\n",
    "Applying scaling to numerical features, while leaving boolean features untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_COLUMNS = [\n",
    "    'age', 'time_in_hospital', 'num_medications',\n",
    "    'num_emergency', 'num_inpatient', 'num_diagnoses'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[NUMERICAL_COLUMNS] = scaler.fit_transform(X_train[NUMERICAL_COLUMNS])\n",
    "X_test[NUMERICAL_COLUMNS] = scaler.transform(X_test[NUMERICAL_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train set:', Counter(y_train))\n",
    "print('Test set:', Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Optuna Hyper-parameter Tuning | No under/oversampling**\n",
    "Hyper-parameter tuning of the model using Optuna, with no undersampling/oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Objective & study setup\n",
    "Preparing the objective function with all hyper-parameters, and creating/loading the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        \"C\": trial.suggest_loguniform(\"C\", 1e-3, 3),\n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        \"degree\": trial.suggest_int(\"degree\", 2, 5) if trial.params.get(\"kernel\", \"\") == \"poly\" else 3,\n",
    "        \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]),\n",
    "        \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "    }\n",
    "\n",
    "    # Define validation folds\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    model = SVC(**params, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kf, scoring=SCORING)\n",
    "\n",
    "    print(\"Cross validation scores: {}\".format(scores))\n",
    "    print(\"Average score: {}\".format(scores.mean()))\n",
    "\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up study with name and storage\n",
    "storage = JournalStorage(JournalFileBackend(STUDY_PATH_NONE))\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(constant_liar=True, seed=RANDOM_STATE),\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyper-parameter tuning\n",
    "**(CAUTION)** <u>Do not run</u>, unless you already need to find the best parameters. If you already have bound the best model configuration, <u>run the section above</u> _(Objective & study setup)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Trial: {}'.format(study.best_trial.number))\n",
    "print('Best Parameters: {}'.format(study.best_params))\n",
    "print('Best Value: {}'.format(study.best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optuna Visualizations\n",
    "Visualizing hyper-parameter tuning results obtained by running Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_timeline(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fitting Best Model\n",
    "Fitting the final model using the best hyper-parameters found by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on the training set\n",
    "best_params = study.best_params\n",
    "\n",
    "# Ensure the solver is compatible with the penalty\n",
    "if best_params.get(\"penalty\") == \"elasticnet\":\n",
    "    best_params[\"solver\"] = \"saga\"\n",
    "\n",
    "model = SVC(**best_params, random_state=RANDOM_STATE)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_none = model.predict(X_train)\n",
    "y_test_pred_none = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_none = accuracy_score(y_train, y_train_pred_none)\n",
    "ac_test_none = accuracy_score(y_test, y_test_pred_none)\n",
    "\n",
    "print('Train accuracy: ', ac_train_none)\n",
    "print('Test accuracy: ', ac_test_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision, recall, F1-score\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred_none,\n",
    "    target_names=['Late or non-readmission', 'Early-readmission']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F1-score, ROC-AUC-score, and accuracy\n",
    "recall_none = recall_score(y_test, y_test_pred_none, average='macro')\n",
    "precision_none = precision_score(y_test, y_test_pred_none, average='macro')\n",
    "f1_none = f1_score(y_test, y_test_pred_none, average='macro')\n",
    "roc_auc_none = roc_auc_score(y_test, y_test_pred_none, average='macro')\n",
    "accuracy_none = accuracy_score(y_test, y_test_pred_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with both absolute and percentage values\n",
    "cm_none = confusion_matrix(y_test, y_test_pred_none)\n",
    "cm_none_norm = confusion_matrix(y_test, y_test_pred_none, normalize='true')\n",
    "\n",
    "annotations_none = np.array([\n",
    "    [f\"{val}\\n({perc:.1%})\" for val, perc in zip(row, norm_row)]\n",
    "    for row, norm_row in zip(cm_none, cm_none_norm)\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_none,\n",
    "    annot=annotations_none,\n",
    "    cmap='Blues',\n",
    "    fmt='',\n",
    "    xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "    yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Optuna Hyper-parameter Tuning | Undersampling**\n",
    "Hyper-parameter tuning of the model using Optuna, and undersampling with RandomUnderSampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Objective & study setup\n",
    "Preparing the objective function with all hyper-parameters, and creating/loading the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    params = {\n",
    "        \"C\": trial.suggest_loguniform(\"C\", 1e-3, 3),\n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        \"degree\": trial.suggest_int(\"degree\", 2, 5) if trial.params.get(\"kernel\", \"\") == \"poly\" else 3,\n",
    "        \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]),\n",
    "        \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "    }\n",
    "    \n",
    "    # Build pipeline\n",
    "    pipeline = make_pipeline(\n",
    "        RandomUnderSampler(random_state=RANDOM_STATE),\n",
    "        SVC(**params, random_state=RANDOM_STATE)\n",
    "    )\n",
    "    \n",
    "    # Define validation folds\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring=SCORING)\n",
    "\n",
    "    print(\"Cross validation scores: {}\".format(scores))\n",
    "    print(\"Average score: {}\".format(scores.mean()))\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up study with name and storage\n",
    "storage = JournalStorage(JournalFileBackend(STUDY_PATH_RUS))\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(constant_liar=True, seed=RANDOM_STATE),\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hyper-parameter tuning\n",
    "**(CAUTION)** <u>Do not run</u>, unless you already need to find the best parameters. If you already have bound the best model configuration, <u>run the section above</u> _(Objective & study setup)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Trial: {}'.format(study.best_trial.number))\n",
    "print('Best Parameters: {}'.format(study.best_params))\n",
    "print('Best Value: {}'.format(study.best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Optuna Visualizations\n",
    "Visualizing hyper-parameter tuning results obtained by running Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_timeline(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Fitting Best Model\n",
    "Fitting the final model using the best hyper-parameters found by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on the training set\n",
    "best_params = study.best_params\n",
    "\n",
    "# Ensure the solver is compatible with the penalty\n",
    "if best_params.get(\"penalty\") == \"elasticnet\":\n",
    "    best_params[\"solver\"] = \"saga\"\n",
    "\n",
    "pipeline_rus = make_pipeline(\n",
    "    RandomUnderSampler(random_state=RANDOM_STATE),\n",
    "    SVC(**best_params, random_state=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "pipeline_rus.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_rus = pipeline_rus.predict(X_train)\n",
    "y_test_pred_rus = pipeline_rus.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_rus = accuracy_score(y_train, y_train_pred_rus)\n",
    "ac_test_rus = accuracy_score(y_test, y_test_pred_rus)\n",
    "\n",
    "print('Train accuracy: ', ac_train_rus)\n",
    "print('Test accuracy: ', ac_test_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision, recall, F1-score\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred_rus,\n",
    "    target_names=['Late or non-readmission', 'Early-readmission']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F1-score, ROC-AUC-score, and accuracy\n",
    "recall_rus = recall_score(y_test, y_test_pred_rus, average='macro')\n",
    "precision_rus = precision_score(y_test, y_test_pred_rus, average='macro')\n",
    "f1_rus = f1_score(y_test, y_test_pred_rus, average='macro')\n",
    "roc_auc_rus = roc_auc_score(y_test, y_test_pred_rus, average='macro')\n",
    "accuracy_rus = accuracy_score(y_test, y_test_pred_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with both absolute and percentage values\n",
    "cm_rus = confusion_matrix(y_test, y_test_pred_rus)\n",
    "cm_rus_norm = confusion_matrix(y_test, y_test_pred_rus, normalize='true')\n",
    "\n",
    "annotations_rus = np.array([\n",
    "    [f\"{val}\\n({perc:.1%})\" for val, perc in zip(row, norm_row)]\n",
    "    for row, norm_row in zip(cm_rus, cm_rus_norm)\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_rus,\n",
    "    annot=annotations_rus,\n",
    "    cmap='Blues',\n",
    "    fmt='',\n",
    "    xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "    yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Optuna Hyper-parameter Tuning | SMOTE**\n",
    "Hyper-parameter tuning of the model using Optuna, and oversampling with SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Objective & study setup\n",
    "Preparing the objective function with all hyper-parameters, and creating/loading the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        \"C\": trial.suggest_loguniform(\"C\", 1e-3, 3),\n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        \"degree\": trial.suggest_int(\"degree\", 2, 5) if trial.params.get(\"kernel\", \"\") == \"poly\" else 3,\n",
    "        \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]),\n",
    "        \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "    }\n",
    "    \n",
    "    # Build pipeline\n",
    "    pipeline = make_pipeline(\n",
    "        SMOTE(random_state=RANDOM_STATE),\n",
    "        SVC(**params, random_state=RANDOM_STATE)\n",
    "    )\n",
    "    \n",
    "    # Define validation folds\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring=SCORING)\n",
    "\n",
    "    print(\"Cross validation scores: {}\".format(scores))\n",
    "    print(\"Average score: {}\".format(scores.mean()))\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up study with name and storage\n",
    "storage = JournalStorage(JournalFileBackend(STUDY_PATH_SMOTE))\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(constant_liar=True, seed=RANDOM_STATE),\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hyper-parameter tuning\n",
    "**(CAUTION)** <u>Do not run</u>, unless you already need to find the best parameters. If you already have bound the best model configuration, <u>run the section above</u> _(Objective & study setup)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Trial: {}'.format(study.best_trial.number))\n",
    "print('Best Parameters: {}'.format(study.best_params))\n",
    "print('Best Value: {}'.format(study.best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Optuna Visualizations\n",
    "Visualizing hyper-parameter tuning results obtained by running Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_timeline(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Fitting Best Model\n",
    "Fitting the final model using the best hyper-parameters found by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on the training set\n",
    "best_params = study.best_params\n",
    "\n",
    "# Ensure the solver is compatible with the penalty\n",
    "if best_params.get(\"penalty\") == \"elasticnet\":\n",
    "    best_params[\"solver\"] = \"saga\"\n",
    "    \n",
    "pipeline_smote = make_pipeline(\n",
    "    SMOTE(random_state=RANDOM_STATE),\n",
    "    SVC(**best_params, random_state=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "pipeline_smote.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_smote = pipeline_smote.predict(X_train)\n",
    "y_test_pred_smote = pipeline_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_smote = accuracy_score(y_train, y_train_pred_smote)\n",
    "ac_test_smote = accuracy_score(y_test, y_test_pred_smote)\n",
    "\n",
    "print('Train accuracy: ', ac_train_smote)\n",
    "print('Test accuracy: ', ac_test_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision, recall, F1-score\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred_smote,\n",
    "    target_names=['Late or non-readmission', 'Early-readmission']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F1-score, ROC-AUC-score, and accuracy\n",
    "recall_smote = recall_score(y_test, y_test_pred_smote, average='macro')\n",
    "precision_smote = precision_score(y_test, y_test_pred_smote, average='macro')\n",
    "f1_smote = f1_score(y_test, y_test_pred_smote, average='macro')\n",
    "roc_auc_smote = roc_auc_score(y_test, y_test_pred_smote, average='macro')\n",
    "accuracy_smote = accuracy_score(y_test, y_test_pred_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with both absolute and percentage values\n",
    "cm_smote = confusion_matrix(y_test, y_test_pred_smote)\n",
    "cm_smote_norm = confusion_matrix(y_test, y_test_pred_smote, normalize='true')\n",
    "\n",
    "annotations_smote = np.array([\n",
    "    [f\"{val}\\n({perc:.1%})\" for val, perc in zip(row, norm_row)]\n",
    "    for row, norm_row in zip(cm_smote, cm_smote_norm)\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_smote,\n",
    "    annot=annotations_smote,\n",
    "    cmap='Blues',\n",
    "    fmt='',\n",
    "    xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "    yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Optuna Hyper-parameter Tuning | SMOTE + Tomek Links**\n",
    "Hyper-parameter tuning of the model using Optuna, by oversampling with SMOTE and undersampling with Tomek Links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Objective & study setup\n",
    "Preparing the objective function with all hyper-parameters, and creating/loading the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        \"C\": trial.suggest_loguniform(\"C\", 1e-3, 3),\n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        \"degree\": trial.suggest_int(\"degree\", 2, 5) if trial.params.get(\"kernel\", \"\") == \"poly\" else 3,\n",
    "        \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]),\n",
    "        \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "    }\n",
    "    \n",
    "    # Build pipeline\n",
    "    pipeline = make_pipeline(\n",
    "        SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=RANDOM_STATE),\n",
    "        SVC(**params, random_state=RANDOM_STATE)\n",
    "    )\n",
    "    \n",
    "    # Define validation folds\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring=SCORING)\n",
    "\n",
    "    print(\"Cross validation scores: {}\".format(scores))\n",
    "    print(\"Average score: {}\".format(scores.mean()))\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up study with name and storage\n",
    "storage = JournalStorage(JournalFileBackend(STUDY_PATH_SMOTE_TOMEK))\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(constant_liar=True, seed=RANDOM_STATE),\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hyper-parameter tuning\n",
    "**(CAUTION)** <u>Do not run</u>, unless you already need to find the best parameters. If you already have bound the best model configuration, <u>run the section above</u> _(Objective & study setup)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Trial: {}'.format(study.best_trial.number))\n",
    "print('Best Parameters: {}'.format(study.best_params))\n",
    "print('Best Value: {}'.format(study.best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Optuna Visualizations\n",
    "Visualizing hyper-parameter tuning results obtained by running Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_timeline(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Fitting Best Model\n",
    "Fitting the final model using the best hyper-parameters found by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on the training set\n",
    "best_params = study.best_params\n",
    "\n",
    "# Ensure the solver is compatible with the penalty\n",
    "if best_params.get(\"penalty\") == \"elasticnet\":\n",
    "    best_params[\"solver\"] = \"saga\"\n",
    "    \n",
    "pipeline_smotetomek = make_pipeline(\n",
    "    SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=RANDOM_STATE),\n",
    "    SVC(**best_params, random_state=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "pipeline_smotetomek.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_smotetomek = pipeline_smotetomek.predict(X_train)\n",
    "y_test_pred_smotetomek = pipeline_smotetomek.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_smotetomek = accuracy_score(y_train, y_train_pred_smotetomek)\n",
    "ac_test_smotetomek = accuracy_score(y_test, y_test_pred_smotetomek)\n",
    "\n",
    "print('Train accuracy: ', ac_train_smotetomek)\n",
    "print('Test accuracy: ', ac_test_smotetomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision, recall, F1-score\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred_smotetomek,\n",
    "    target_names=['Late or non-readmission', 'Early-readmission']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F1-score, ROC-AUC-score, and accuracy\n",
    "recall_smotetomek = recall_score(y_test, y_test_pred_smotetomek, average='macro')\n",
    "precision_smotetomek = precision_score(y_test, y_test_pred_smotetomek, average='macro')\n",
    "f1_smotetomek = f1_score(y_test, y_test_pred_smotetomek, average='macro')\n",
    "roc_auc_smotetomek = roc_auc_score(y_test, y_test_pred_smotetomek, average='macro')\n",
    "accuracy_smotetomek = accuracy_score(y_test, y_test_pred_smotetomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with both absolute and percentage values\n",
    "cm_smotetomek = confusion_matrix(y_test, y_test_pred_smotetomek)\n",
    "cm_smotetomek_norm = confusion_matrix(y_test, y_test_pred_smotetomek, normalize='true')\n",
    "\n",
    "annotations_smotetomek = np.array([\n",
    "    [f\"{val}\\n({perc:.1%})\" for val, perc in zip(row, norm_row)]\n",
    "    for row, norm_row in zip(cm_smotetomek, cm_smotetomek_norm)\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_smotetomek,\n",
    "    annot=annotations_smotetomek,\n",
    "    cmap='Blues',\n",
    "    fmt='',\n",
    "    xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "    yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Results Combined**\n",
    "Visualizing all results obtained by different sampling methods, on the best respective models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all confusion matrices into a single plot\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "cm_list = [cm_none_norm, cm_rus_norm, cm_smote_norm, cm_smotetomek_norm]\n",
    "annotations = [annotations_none, annotations_rus, annotations_smote, annotations_smotetomek]\n",
    "labels = ['No Under/Oversampling', 'Random Undersampling', 'SMOTE', 'SMOTE-Tomek']\n",
    "\n",
    "# Define tick labels\n",
    "xticklabels = ['Late or non-readmission', 'Early-readmission']\n",
    "yticklabels = ['Late or non-readmission', 'Early-readmission']\n",
    "\n",
    "for i, cm in enumerate(cm_list):\n",
    "    ax=axs[i//2, i%2]\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=annotations[i],\n",
    "        cmap='Blues',\n",
    "        fmt='',\n",
    "        vmin=0, vmax=1,\n",
    "        ax=axs[i//2, i%2],\n",
    "        xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "        yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    "    )\n",
    "    ax.set_title(labels[i])\n",
    "    \n",
    "    # Hide x-tick labels for the top row\n",
    "    if i < 2:\n",
    "        ax.set_xticklabels([])\n",
    "    # Hide y-tick labels for the right column\n",
    "    if i % 2 == 1:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the scores for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all metrics into a single DataFrame\n",
    "data = {\n",
    "    'Recall': [recall_none, recall_rus, recall_smote, recall_smotetomek],\n",
    "    'Precision': [precision_none, precision_rus, precision_smote, precision_smotetomek],\n",
    "    'F1-score': [f1_none, f1_rus, f1_smote, f1_smotetomek],\n",
    "    'ROC-AUC': [roc_auc_none, roc_auc_rus, roc_auc_smote, roc_auc_smotetomek],\n",
    "    'Accuracy': [accuracy_none, accuracy_rus, accuracy_smote, accuracy_smotetomek]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(data, index=labels)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all metrics in a bar plot, by coloring each method differently, using SeaBorn barplot and adding grids\n",
    "sns.set_palette('viridis')\n",
    "metrics_df.plot(kind='bar', figsize=(10, 4))\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
