{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Tuning | Neural Network\n",
    "**Neural Network** hyper-parameter tuning for the [UCI dataset](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008). The dataset represents ten years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks.\n",
    "\n",
    "> **[CAUTION]** Do <u>NOT</u> execute \"Run All\", that will tell Optuna to run hyper-parameter tuning, regardless of whether you have already done it or not. Avoid running the _\"Hyper-parameter tuning\"_ subsections if you don't have to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Initial Setup**\n",
    "Taking care of package imports, defining work constants, and loading all necessary datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "# Machine learning\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualization configurations\n",
    "pd.set_option('display.max_columns', 60)\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, fbeta_score, make_scorer, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/original/'\n",
    "DATA_PATH_PROCESSED = '../data/processed/'\n",
    "\n",
    "MODEL_NAME = 'nn' # Modify this to the model you are using\n",
    "\n",
    "OPTUNA_PATH = '../optuna/'\n",
    "OPTUNA_DIR = OPTUNA_PATH + MODEL_NAME + '/'\n",
    "\n",
    "STUDY_NAME = \"Diabetes_130-US\"\n",
    "STUDY_PATH_NONE = OPTUNA_DIR + f\"optuna_{MODEL_NAME}_none.log\"\n",
    "STUDY_PATH_RUS = OPTUNA_DIR + f\"optuna_{MODEL_NAME}_rus.log\"\n",
    "STUDY_PATH_SMOTE = OPTUNA_DIR + f\"optuna_{MODEL_NAME}_smote.log\"\n",
    "STUDY_PATH_SMOTE_TOMEK = OPTUNA_DIR + f\"optuna_{MODEL_NAME}_smote-tomek.log\"\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(OPTUNA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(STUDY_PATH_NONE), exist_ok=True)\n",
    "\n",
    "N_TRIALS = 100\n",
    "RANDOM_STATE = 38\n",
    "\n",
    "SCORE_NAME = 'F1-Score'\n",
    "SCORING = make_scorer(fbeta_score, beta=1, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\n",
    "    DATA_PATH_PROCESSED + 'train.csv',\n",
    "    na_values='?',\n",
    "    keep_default_na=False\n",
    ")\n",
    "\n",
    "test_set = pd.read_csv(\n",
    "    DATA_PATH_PROCESSED + 'test.csv',\n",
    "    na_values='?',\n",
    "    keep_default_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Feature Scaling**\n",
    "Applying scaling to numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'readmitted'\n",
    "\n",
    "X_train = train_set.drop(target, axis=1)\n",
    "y_train = train_set[target]\n",
    "\n",
    "X_test = test_set.drop(target, axis=1)\n",
    "y_test = test_set[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Scaling\n",
    "Applying scaling to numerical features, while leaving boolean features untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_COLUMNS = [\n",
    "    'age', 'time_in_hospital', 'num_medications',\n",
    "    'num_emergency', 'num_inpatient', 'num_diagnoses'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[NUMERICAL_COLUMNS] = scaler.fit_transform(X_train[NUMERICAL_COLUMNS])\n",
    "X_test[NUMERICAL_COLUMNS] = scaler.transform(X_test[NUMERICAL_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Counter({0: 45522, 1: 4523})\n",
      "Test set: Counter({0: 19510, 1: 1938})\n"
     ]
    }
   ],
   "source": [
    "print('Train set:', Counter(y_train))\n",
    "print('Test set:', Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Optuna Hyper-parameter Tuning | No under/oversampling**\n",
    "Hyper-parameter tuning of the model using Optuna, with no undersampling/oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Objective & study setup\n",
    "Preparing the objective function with all hyper-parameters, and creating/loading the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        \"hidden_layer_sizes\": tuple(\n",
    "            trial.suggest_int(f\"layer_{i}\", 32, 128) for i in range(trial.suggest_int(\"n_layers\", 1, 2))\n",
    "        ),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", [\"tanh\", \"relu\"]),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"]),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1e-2, log=True),\n",
    "        \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\", 1e-3, 1e-2, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64]),\n",
    "        \"max_iter\": 500,\n",
    "        \"early_stopping\": True,\n",
    "        \"n_iter_no_change\": 5,\n",
    "        \"tol\": 1e-4\n",
    "    }\n",
    "    \n",
    "    # Define validation folds\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    model = MLPClassifier(**params, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kf, scoring=SCORING)\n",
    "\n",
    "    print(\"Cross validation scores: {}\".format(scores))\n",
    "    print(\"Average score: {}\".format(scores.mean()))\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\optuna\\_experimental.py:31: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2025-01-14 17:10:05,596] A new study created in Journal with name: Diabetes_130-US\n"
     ]
    }
   ],
   "source": [
    "# Set up study with name and storage\n",
    "storage = JournalStorage(JournalFileBackend(STUDY_PATH_NONE))\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(constant_liar=True, seed=RANDOM_STATE),\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyper-parameter tuning\n",
    "**(CAUTION)** <u>Do not run</u>, unless you already need to find the best parameters. If you already have bound the best model configuration, <u>run the section above</u> _(Objective & study setup)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 17:12:04,960] Trial 5 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0001836940535741272, 'learning_rate_init': 0.00966087287341526, 'batch_size': 64}. Best is trial 5 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 17:12:58,513] Trial 6 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 86, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0008679566255102244, 'learning_rate_init': 0.0054929669269487115, 'batch_size': 64}. Best is trial 5 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:14:11,300] Trial 0 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 69, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0009713289727408835, 'learning_rate_init': 0.008390808709979321, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:14:35,967] Trial 7 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 112, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0007498816470606433, 'learning_rate_init': 0.009877593219983612, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:17:03,573] Trial 11 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 98, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0004438740807487775, 'learning_rate_init': 0.0020709969628716685, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:17:42,931] Trial 1 finished with value: 0.47633597337075617 and parameters: {'n_layers': 2, 'layer_0': 72, 'layer_1': 118, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00017452771106961205, 'learning_rate_init': 0.0089212179071488, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:18:28,064] Trial 9 finished with value: 0.47633597337075617 and parameters: {'n_layers': 2, 'layer_0': 53, 'layer_1': 96, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0004455545317292566, 'learning_rate_init': 0.002303513898159477, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 17:20:12,488] Trial 13 finished with value: 0.47633597337075617 and parameters: {'n_layers': 2, 'layer_0': 85, 'layer_1': 52, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.00032172062240894204, 'learning_rate_init': 0.004200464292587725, 'batch_size': 64}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 17:20:21,834] Trial 14 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 79, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00013989376307742973, 'learning_rate_init': 0.008725397288720519, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:22:23,553] Trial 15 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 104, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0069212175089938014, 'learning_rate_init': 0.004810432726355153, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 17:22:42,509] Trial 16 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 113, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0012239961843117189, 'learning_rate_init': 0.002320256528424692, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:24:19,937] Trial 17 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 42, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0026012656060123438, 'learning_rate_init': 0.006506762350073762, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:26:47,609] Trial 18 finished with value: 0.47633597337075617 and parameters: {'n_layers': 2, 'layer_0': 67, 'layer_1': 95, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.001732515400159403, 'learning_rate_init': 0.006893595429323319, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 17:27:45,285] Trial 19 finished with value: 0.47633597337075617 and parameters: {'n_layers': 2, 'layer_0': 69, 'layer_1': 67, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0037324169961025482, 'learning_rate_init': 0.0010721944929978657, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:29:09,653] Trial 20 finished with value: 0.47633597337075617 and parameters: {'n_layers': 2, 'layer_0': 32, 'layer_1': 34, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00023910665451904134, 'learning_rate_init': 0.007005972915660694, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 17:29:36,641] Trial 21 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 93, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0016218160167980752, 'learning_rate_init': 0.007186502203029271, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:30:47,213] Trial 22 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 73, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.004906616063751442, 'learning_rate_init': 0.005797232175400292, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 17:32:26,101] Trial 24 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 41, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0019915294602405316, 'learning_rate_init': 0.003984031774680096, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 17:33:40,611] Trial 23 finished with value: 0.47633597337075617 and parameters: {'n_layers': 2, 'layer_0': 58, 'layer_1': 96, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00028546784501390756, 'learning_rate_init': 0.004601513606779774, 'batch_size': 32}. Best is trial 0 with value: 0.47633597337075617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:34:25,043] Trial 4 finished with value: 0.4885182404074332 and parameters: {'n_layers': 1, 'layer_0': 128, 'activation': 'relu', 'solver': 'lbfgs', 'alpha': 0.0004853240478615673, 'learning_rate_init': 0.008637531517358923, 'batch_size': 64}. Best is trial 4 with value: 0.4885182404074332.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.48521968 0.49620654 0.48661286 0.48556345 0.48898866]\n",
      "Average score: 0.4885182404074332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:35:33,290] Trial 26 finished with value: 0.47633597337075617 and parameters: {'n_layers': 1, 'layer_0': 94, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0002207759821128697, 'learning_rate_init': 0.001355143784640353, 'batch_size': 32}. Best is trial 4 with value: 0.4885182404074332.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:36:36,813] Trial 25 finished with value: 0.47633597337075617 and parameters: {'n_layers': 2, 'layer_0': 73, 'layer_1': 108, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.001274013558572769, 'learning_rate_init': 0.008192333285334736, 'batch_size': 32}. Best is trial 4 with value: 0.4885182404074332.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.47635241 0.47635241 0.47632501 0.47632501 0.47632501]\n",
      "Average score: 0.47633597337075617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:44:42,756] Trial 3 finished with value: 0.5031063026254697 and parameters: {'n_layers': 2, 'layer_0': 80, 'layer_1': 52, 'activation': 'relu', 'solver': 'lbfgs', 'alpha': 0.0004089232955782988, 'learning_rate_init': 0.005136507545558117, 'batch_size': 64}. Best is trial 3 with value: 0.5031063026254697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.50346578 0.50742818 0.50497338 0.51113376 0.4885304 ]\n",
      "Average score: 0.5031063026254697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:49:19,011] Trial 8 finished with value: 0.5013437831096371 and parameters: {'n_layers': 2, 'layer_0': 61, 'layer_1': 83, 'activation': 'relu', 'solver': 'lbfgs', 'alpha': 0.007935103543076631, 'learning_rate_init': 0.003624178251304356, 'batch_size': 64}. Best is trial 3 with value: 0.5031063026254697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.50064072 0.50507435 0.50004959 0.50116858 0.49978568]\n",
      "Average score: 0.5013437831096371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[I 2025-01-14 17:50:08,251] Trial 2 finished with value: 0.5030724449602155 and parameters: {'n_layers': 2, 'layer_0': 51, 'layer_1': 126, 'activation': 'relu', 'solver': 'lbfgs', 'alpha': 0.000623229673647507, 'learning_rate_init': 0.0032792437328772956, 'batch_size': 64}. Best is trial 3 with value: 0.5031063026254697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores: [0.50590881 0.50683261 0.49791283 0.5048696  0.49983838]\n",
      "Average score: 0.5030724449602155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\alero\\.vscode\\unibs_mldm_diabetes\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Trial: {}'.format(study.best_trial.number))\n",
    "print('Best Parameters: {}'.format(study.best_params))\n",
    "print('Best Value: {}'.format(study.best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optuna Visualizations\n",
    "Visualizing hyper-parameter tuning results obtained by running Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_timeline(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fitting Best Model\n",
    "Fitting the final model using the best hyper-parameters found by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on the training set\n",
    "best_params = study.best_params\n",
    "\n",
    "model = MLPClassifier(**best_params, random_state=RANDOM_STATE)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_none = model.predict(X_train)\n",
    "y_test_pred_none = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_none = accuracy_score(y_train, y_train_pred_none)\n",
    "ac_test_none = accuracy_score(y_test, y_test_pred_none)\n",
    "\n",
    "print('Train accuracy: ', ac_train_none)\n",
    "print('Test accuracy: ', ac_test_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision, recall, F1-score\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred_none,\n",
    "    target_names=['Late or non-readmission', 'Early-readmission']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F1-score, ROC-AUC-score, and accuracy\n",
    "recall_none = recall_score(y_test, y_test_pred_none, average='macro')\n",
    "precision_none = precision_score(y_test, y_test_pred_none, average='macro')\n",
    "f1_none = f1_score(y_test, y_test_pred_none, average='macro')\n",
    "roc_auc_none = roc_auc_score(y_test, y_test_pred_none, average='macro')\n",
    "accuracy_none = accuracy_score(y_test, y_test_pred_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with both absolute and percentage values\n",
    "cm_none = confusion_matrix(y_test, y_test_pred_none)\n",
    "cm_none_norm = confusion_matrix(y_test, y_test_pred_none, normalize='true')\n",
    "\n",
    "annotations_none = np.array([\n",
    "    [f\"{val}\\n({perc:.1%})\" for val, perc in zip(row, norm_row)]\n",
    "    for row, norm_row in zip(cm_none, cm_none_norm)\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_none,\n",
    "    annot=annotations_none,\n",
    "    cmap='Blues',\n",
    "    fmt='',\n",
    "    xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "    yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Optuna Hyper-parameter Tuning | Undersampling**\n",
    "Hyper-parameter tuning of the model using Optuna, and undersampling with RandomUnderSampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Objective & study setup\n",
    "Preparing the objective function with all hyper-parameters, and creating/loading the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        \"hidden_layer_sizes\": tuple(\n",
    "            trial.suggest_int(f\"layer_{i}\", 10, 100) for i in range(trial.suggest_int(\"n_layers\", 1, 3))\n",
    "        ),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", [\"identity\", \"logistic\", \"tanh\", \"relu\"]),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"]),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-5, 1e-1, log=True),\n",
    "        \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\", 1e-4, 1e-1, log=True),\n",
    "        \"max_iter\": 500\n",
    "    }\n",
    "    \n",
    "    # Build pipeline\n",
    "    pipeline = make_pipeline(\n",
    "        RandomUnderSampler(random_state=RANDOM_STATE),\n",
    "        MLPClassifier(**params, random_state=RANDOM_STATE)\n",
    "    )\n",
    "    \n",
    "    # Define validation folds\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring=SCORING)\n",
    "\n",
    "    print(\"Cross validation scores: {}\".format(scores))\n",
    "    print(\"Average score: {}\".format(scores.mean()))\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up study with name and storage\n",
    "storage = JournalStorage(JournalFileBackend(STUDY_PATH_RUS))\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(constant_liar=True, seed=RANDOM_STATE),\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hyper-parameter tuning\n",
    "**(CAUTION)** <u>Do not run</u>, unless you already need to find the best parameters. If you already have bound the best model configuration, <u>run the section above</u> _(Objective & study setup)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Trial: {}'.format(study.best_trial.number))\n",
    "print('Best Parameters: {}'.format(study.best_params))\n",
    "print('Best Value: {}'.format(study.best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Optuna Visualizations\n",
    "Visualizing hyper-parameter tuning results obtained by running Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_timeline(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Fitting Best Model\n",
    "Fitting the final model using the best hyper-parameters found by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on the training set\n",
    "best_params = study.best_params\n",
    "\n",
    "pipeline_rus = make_pipeline(\n",
    "    RandomUnderSampler(random_state=RANDOM_STATE),\n",
    "    MLPClassifier(**best_params, random_state=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "pipeline_rus.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_rus = pipeline_rus.predict(X_train)\n",
    "y_test_pred_rus = pipeline_rus.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_rus = accuracy_score(y_train, y_train_pred_rus)\n",
    "ac_test_rus = accuracy_score(y_test, y_test_pred_rus)\n",
    "\n",
    "print('Train accuracy: ', ac_train_rus)\n",
    "print('Test accuracy: ', ac_test_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision, recall, F1-score\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred_rus,\n",
    "    target_names=['Late or non-readmission', 'Early-readmission']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F1-score, ROC-AUC-score, and accuracy\n",
    "recall_rus = recall_score(y_test, y_test_pred_rus, average='macro')\n",
    "precision_rus = precision_score(y_test, y_test_pred_rus, average='macro')\n",
    "f1_rus = f1_score(y_test, y_test_pred_rus, average='macro')\n",
    "roc_auc_rus = roc_auc_score(y_test, y_test_pred_rus, average='macro')\n",
    "accuracy_rus = accuracy_score(y_test, y_test_pred_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with both absolute and percentage values\n",
    "cm_rus = confusion_matrix(y_test, y_test_pred_rus)\n",
    "cm_rus_norm = confusion_matrix(y_test, y_test_pred_rus, normalize='true')\n",
    "\n",
    "annotations_rus = np.array([\n",
    "    [f\"{val}\\n({perc:.1%})\" for val, perc in zip(row, norm_row)]\n",
    "    for row, norm_row in zip(cm_rus, cm_rus_norm)\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_rus,\n",
    "    annot=annotations_rus,\n",
    "    cmap='Blues',\n",
    "    fmt='',\n",
    "    xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "    yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Optuna Hyper-parameter Tuning | SMOTE**\n",
    "Hyper-parameter tuning of the model using Optuna, and oversampling with SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Objective & study setup\n",
    "Preparing the objective function with all hyper-parameters, and creating/loading the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        \"hidden_layer_sizes\": tuple(\n",
    "            trial.suggest_int(f\"layer_{i}\", 10, 100) for i in range(trial.suggest_int(\"n_layers\", 1, 3))\n",
    "        ),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", [\"identity\", \"logistic\", \"tanh\", \"relu\"]),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"]),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-5, 1e-1, log=True),\n",
    "        \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\", 1e-4, 1e-1, log=True),\n",
    "        \"max_iter\": 500\n",
    "    }\n",
    "    \n",
    "    # Build pipeline\n",
    "    pipeline = make_pipeline(\n",
    "        SMOTE(random_state=RANDOM_STATE),\n",
    "        MLPClassifier(**params, random_state=RANDOM_STATE)\n",
    "    )\n",
    "    \n",
    "    # Define validation folds\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring=SCORING)\n",
    "\n",
    "    print(\"Cross validation scores: {}\".format(scores))\n",
    "    print(\"Average score: {}\".format(scores.mean()))\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up study with name and storage\n",
    "storage = JournalStorage(JournalFileBackend(STUDY_PATH_SMOTE))\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(constant_liar=True, seed=RANDOM_STATE),\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hyper-parameter tuning\n",
    "**(CAUTION)** <u>Do not run</u>, unless you already need to find the best parameters. If you already have bound the best model configuration, <u>run the section above</u> _(Objective & study setup)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Trial: {}'.format(study.best_trial.number))\n",
    "print('Best Parameters: {}'.format(study.best_params))\n",
    "print('Best Value: {}'.format(study.best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Optuna Visualizations\n",
    "Visualizing hyper-parameter tuning results obtained by running Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_timeline(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Fitting Best Model\n",
    "Fitting the final model using the best hyper-parameters found by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on the training set\n",
    "best_params = study.best_params\n",
    "\n",
    "pipeline_smote = make_pipeline(\n",
    "    SMOTE(random_state=RANDOM_STATE),\n",
    "    MLPClassifier(**best_params, random_state=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "pipeline_smote.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_smote = pipeline_smote.predict(X_train)\n",
    "y_test_pred_smote = pipeline_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_smote = accuracy_score(y_train, y_train_pred_smote)\n",
    "ac_test_smote = accuracy_score(y_test, y_test_pred_smote)\n",
    "\n",
    "print('Train accuracy: ', ac_train_smote)\n",
    "print('Test accuracy: ', ac_test_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision, recall, F1-score\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred_smote,\n",
    "    target_names=['Late or non-readmission', 'Early-readmission']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F1-score, ROC-AUC-score, and accuracy\n",
    "recall_smote = recall_score(y_test, y_test_pred_smote, average='macro')\n",
    "precision_smote = precision_score(y_test, y_test_pred_smote, average='macro')\n",
    "f1_smote = f1_score(y_test, y_test_pred_smote, average='macro')\n",
    "roc_auc_smote = roc_auc_score(y_test, y_test_pred_smote, average='macro')\n",
    "accuracy_smote = accuracy_score(y_test, y_test_pred_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with both absolute and percentage values\n",
    "cm_smote = confusion_matrix(y_test, y_test_pred_smote)\n",
    "cm_smote_norm = confusion_matrix(y_test, y_test_pred_smote, normalize='true')\n",
    "\n",
    "annotations_smote = np.array([\n",
    "    [f\"{val}\\n({perc:.1%})\" for val, perc in zip(row, norm_row)]\n",
    "    for row, norm_row in zip(cm_smote, cm_smote_norm)\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_smote,\n",
    "    annot=annotations_smote,\n",
    "    cmap='Blues',\n",
    "    fmt='',\n",
    "    xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "    yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Optuna Hyper-parameter Tuning | SMOTE + Tomek Links**\n",
    "Hyper-parameter tuning of the model using Optuna, by oversampling with SMOTE and undersampling with Tomek Links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Objective & study setup\n",
    "Preparing the objective function with all hyper-parameters, and creating/loading the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        \"hidden_layer_sizes\": tuple(\n",
    "            trial.suggest_int(f\"layer_{i}\", 10, 100) for i in range(trial.suggest_int(\"n_layers\", 1, 3))\n",
    "        ),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", [\"identity\", \"logistic\", \"tanh\", \"relu\"]),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"]),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-5, 1e-1, log=True),\n",
    "        \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\", 1e-4, 1e-1, log=True),\n",
    "        \"max_iter\": 500\n",
    "    }\n",
    "    \n",
    "    # Build pipeline\n",
    "    pipeline = make_pipeline(\n",
    "        SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=RANDOM_STATE),\n",
    "        MLPClassifier(**params, random_state=RANDOM_STATE)\n",
    "    )\n",
    "    \n",
    "    # Define validation folds\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring=SCORING)\n",
    "\n",
    "    print(\"Cross validation scores: {}\".format(scores))\n",
    "    print(\"Average score: {}\".format(scores.mean()))\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up study with name and storage\n",
    "storage = JournalStorage(JournalFileBackend(STUDY_PATH_SMOTE_TOMEK))\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(constant_liar=True, seed=RANDOM_STATE),\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hyper-parameter tuning\n",
    "**(CAUTION)** <u>Do not run</u>, unless you already need to find the best parameters. If you already have bound the best model configuration, <u>run the section above</u> _(Objective & study setup)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Trial: {}'.format(study.best_trial.number))\n",
    "print('Best Parameters: {}'.format(study.best_params))\n",
    "print('Best Value: {}'.format(study.best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Optuna Visualizations\n",
    "Visualizing hyper-parameter tuning results obtained by running Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_timeline(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Fitting Best Model\n",
    "Fitting the final model using the best hyper-parameters found by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on the training set\n",
    "best_params = study.best_params\n",
    "\n",
    "pipeline_smotetomek = make_pipeline(\n",
    "    SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=RANDOM_STATE),\n",
    "    MLPClassifier(**best_params, random_state=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "pipeline_smotetomek.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_smotetomek = pipeline_smotetomek.predict(X_train)\n",
    "y_test_pred_smotetomek = pipeline_smotetomek.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_smotetomek = accuracy_score(y_train, y_train_pred_smotetomek)\n",
    "ac_test_smotetomek = accuracy_score(y_test, y_test_pred_smotetomek)\n",
    "\n",
    "print('Train accuracy: ', ac_train_smotetomek)\n",
    "print('Test accuracy: ', ac_test_smotetomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision, recall, F1-score\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred_smotetomek,\n",
    "    target_names=['Late or non-readmission', 'Early-readmission']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F1-score, ROC-AUC-score, and accuracy\n",
    "recall_smotetomek = recall_score(y_test, y_test_pred_smotetomek, average='macro')\n",
    "precision_smotetomek = precision_score(y_test, y_test_pred_smotetomek, average='macro')\n",
    "f1_smotetomek = f1_score(y_test, y_test_pred_smotetomek, average='macro')\n",
    "roc_auc_smotetomek = roc_auc_score(y_test, y_test_pred_smotetomek, average='macro')\n",
    "accuracy_smotetomek = accuracy_score(y_test, y_test_pred_smotetomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with both absolute and percentage values\n",
    "cm_smotetomek = confusion_matrix(y_test, y_test_pred_smotetomek)\n",
    "cm_smotetomek_norm = confusion_matrix(y_test, y_test_pred_smotetomek, normalize='true')\n",
    "\n",
    "annotations_smotetomek = np.array([\n",
    "    [f\"{val}\\n({perc:.1%})\" for val, perc in zip(row, norm_row)]\n",
    "    for row, norm_row in zip(cm_smotetomek, cm_smotetomek_norm)\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_smotetomek,\n",
    "    annot=annotations_smotetomek,\n",
    "    cmap='Blues',\n",
    "    fmt='',\n",
    "    xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "    yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Results Combined**\n",
    "Visualizing all results obtained by different sampling methods, on the best respective models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all confusion matrices into a single plot\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "cm_list = [cm_none_norm, cm_rus_norm, cm_smote_norm, cm_smotetomek_norm]\n",
    "annotations = [annotations_none, annotations_rus, annotations_smote, annotations_smotetomek]\n",
    "labels = ['No Under/Oversampling', 'Random Undersampling', 'SMOTE', 'SMOTE-Tomek']\n",
    "\n",
    "# Define tick labels\n",
    "xticklabels = ['Late or non-readmission', 'Early-readmission']\n",
    "yticklabels = ['Late or non-readmission', 'Early-readmission']\n",
    "\n",
    "for i, cm in enumerate(cm_list):\n",
    "    ax=axs[i//2, i%2]\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=annotations[i],\n",
    "        cmap='Blues',\n",
    "        fmt='',\n",
    "        vmin=0, vmax=1,\n",
    "        ax=axs[i//2, i%2],\n",
    "        xticklabels=['Late or non-readmission', 'Early-readmission'],\n",
    "        yticklabels=['Late or non-readmission', 'Early-readmission']\n",
    "    )\n",
    "    ax.set_title(labels[i])\n",
    "    \n",
    "    # Hide x-tick labels for the top row\n",
    "    if i < 2:\n",
    "        ax.set_xticklabels([])\n",
    "    # Hide y-tick labels for the right column\n",
    "    if i % 2 == 1:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the scores for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all metrics into a single DataFrame\n",
    "data = {\n",
    "    'Recall': [recall_none, recall_rus, recall_smote, recall_smotetomek],\n",
    "    'Precision': [precision_none, precision_rus, precision_smote, precision_smotetomek],\n",
    "    'F1-score': [f1_none, f1_rus, f1_smote, f1_smotetomek],\n",
    "    'ROC-AUC': [roc_auc_none, roc_auc_rus, roc_auc_smote, roc_auc_smotetomek],\n",
    "    'Accuracy': [accuracy_none, accuracy_rus, accuracy_smote, accuracy_smotetomek]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(data, index=labels)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all metrics in a bar plot, by coloring each method differently, using SeaBorn barplot and adding grids\n",
    "sns.set_palette('viridis')\n",
    "metrics_df.plot(kind='bar', figsize=(10, 4))\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
